{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Check how to limit the maximum depth"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Method 2: using grad from a ReLU network between depth and target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "sys.path.append(\"/home/wukailu/latent-nerf/src/ControlNet\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "mean_std = torch.tensor([\n",
    "    (-0.24595006, 0.54566), # R\n",
    "    (-0.13202806, 0.55846), # G\n",
    "    (-0.02775778, 0.57430), # B\n",
    "    ( 1.72314970, 0.99023), # D\n",
    "])\n",
    "mean, std = mean_std.to(device=device).unbind(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "def get_unet():\n",
    "    unet = UNetModel(image_size=IMG_SIZE,\n",
    "                     in_channels=8, out_channels=4,\n",
    "                     model_channels=128, # the base channel (smallest)\n",
    "                     channel_mult=[1, 2, 3, 3, 4, 4],\n",
    "                     num_res_blocks=2,\n",
    "                     num_head_channels=32,\n",
    "                     # down 1     2      4          8          16         32\n",
    "                     # res  128   64     32         16         8          4\n",
    "                     # chan 128   256    384        384        512        512\n",
    "                     # type conv  conv   conv+attn  conv+attn  conv+attn  conv+attn\n",
    "                     attention_resolutions=[4, 8, 16, 32],\n",
    "                     use_checkpoint=True,\n",
    "                     use_fp16=False,\n",
    "                    )\n",
    "    # use num_groups==1, to avoid color shift problem\n",
    "    for name, module in unet.named_modules():\n",
    "        if isinstance(module, nn.GroupNorm):\n",
    "            module.num_groups = 1\n",
    "            print(f\"convert GN to LN for module: {name}\")\n",
    "    return unet\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.unet = get_unet()\n",
    "        # dummy params\n",
    "        self.no_pixel = nn.Parameter(torch.zeros(4))\n",
    "\n",
    "    def forward(self, rgbd, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgbd.shape == (B, 4, H, W)\n",
    "\n",
    "            NOTE\n",
    "            curr view (i.e. rgbd) is noised\n",
    "        \"\"\"\n",
    "        rgbd_render = torch.zeros_like(rgbd)\n",
    "        unet_in = torch.cat([rgbd_render, rgbd], dim=1) # (B, 4 + 4, H, W)\n",
    "        pred = self.unet(unet_in, t) # (B, C, H, W)\n",
    "        return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load model from dir_ckpt, \"checkpoint\", \"model.pt\"\n",
    "model_path = \"/home/wukailu/RGBD-Diffusion/out/RGBD2/checkpoint/model.pt\"\n",
    "ckpt = torch.load(model_path)\n",
    "model = Model()\n",
    "model.load_state_dict(ckpt['model'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from diffusers import DDIMScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_steps  = 50\n",
    "diffusion_scheduler = DDIMScheduler(num_train_timesteps=1000, clip_sample=False, set_alpha_to_one=False)\n",
    "@torch.no_grad()\n",
    "def sampling_forward_fn(rgbd, t): # forward func only for classifier-free sampling\n",
    "    t = t.reshape([1])\n",
    "    # forward\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        pred = model.unet(rgbd, t)\n",
    "    return pred.float()\n",
    "\n",
    "class Sampler:\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, seed, call_back=None):\n",
    "        # sample noise\n",
    "        kwargs_rand = lambda seed, device=device: dict(generator=torch.Generator(device).manual_seed(seed), device=device)\n",
    "        z_t = torch.randn([1, 4, IMG_SIZE, IMG_SIZE], **kwargs_rand(seed))\n",
    "        # compute the known part by rendering the mesh onto the current view\n",
    "        known_part = torch.zeros((1, 4, IMG_SIZE, IMG_SIZE), device=device)\n",
    "        #\n",
    "        diffusion_scheduler.set_timesteps(num_steps)\n",
    "        #\n",
    "        time_step_lst = diffusion_scheduler.timesteps\n",
    "        assert num_steps == diffusion_scheduler.num_inference_steps == len(time_step_lst)\n",
    "        #\n",
    "        model.eval()\n",
    "        for i, t in tqdm(enumerate(time_step_lst)):\n",
    "            rgbd_in = torch.cat([known_part, z_t], dim=1)  # (1, 8, H, W)\n",
    "            pred_noise = sampling_forward_fn(rgbd_in, t.to(device=device))\n",
    "            if call_back is not None:\n",
    "                call_back(i, t, pred_noise, rgbd_in)\n",
    "            z_t = diffusion_scheduler.step(pred_noise, t.to(device=device), z_t).prev_sample\n",
    "        # reshape to (H, W, C)\n",
    "        rgbd_curr = rearrange(z_t, \"() C H W -> H W C\")\n",
    "        rgbd_curr = rgbd_curr * std + mean\n",
    "        rgbd_curr[..., :3] = (rgbd_curr[..., :3] + 1) / 2 # 0~1\n",
    "        rgbd_curr[..., :3] = rgbd_curr[..., :3].clamp(min=0, max=1)\n",
    "        rgbd_curr[...,  3] = rgbd_curr[...,  3].clamp(min=0)\n",
    "        return rgbd_curr\n",
    "\n",
    "def midas_callback_rgbd(i, t, noise_pred_uncond, model_input):\n",
    "    # TODO: check this\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        with torch.enable_grad():\n",
    "            ltt: torch.Tensor = model_input.detach()\n",
    "            ltt = ltt.requires_grad_(True)\n",
    "\n",
    "            pred = ltt[:, -1]\n",
    "            target = torch.ones_like(pred) * (pred.mean().detach())\n",
    "            loss_func = torch.nn.MSELoss()  # MSE because we assume it follows a normal distribution\n",
    "            loss = loss_func(pred.flatten(start_dim=1), target.flatten(start_dim=1))\n",
    "            loss.backward()\n",
    "\n",
    "        grad = ltt.grad * -0.5 * 3 / (1**2)  # divide sigma^2, if depth distribution follows N(target, sigma)\n",
    "        grad = grad.to(dtype=noise_pred_uncond.dtype)\n",
    "        sqrt_one_minus_alpha_prod = (1 - diffusion_scheduler.alphas_cumprod[t]) ** 0.5\n",
    "        noise_pred_uncond -= sqrt_one_minus_alpha_prod.to(grad) * grad\n",
    "        print(\"grad\", grad.mean(), grad.std(), grad.shape)\n",
    "        print(\"noise_pred_uncond\", noise_pred_uncond.mean(), noise_pred_uncond.std(), noise_pred_uncond.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rgbd = Sampler()(seed=237) # (H, W, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def show_rgbd(rgbd_display):\n",
    "    rgbd_input = rgbd_display.clone()\n",
    "    rgbd_input[..., :3] = rgbd_input[..., :3] * 255\n",
    "    d_max = rgbd_input[..., 3:].max()\n",
    "    threshold = d_max * 0.05\n",
    "    ref = rgbd_input[..., 3].clone()\n",
    "    d_min = ref[ref >= threshold].min()\n",
    "    d_max = ref[ref >= threshold].max()\n",
    "    ref = (ref - d_min).clip(0) / (d_max - d_min) * 255\n",
    "    rgbd_input[..., 3] = ref\n",
    "\n",
    "    pil = Image.fromarray(rgbd_input.numpy().round().astype(np.uint8)[..., :3])\n",
    "    display(pil)\n",
    "    pil = Image.fromarray(rgbd_input.numpy().round().astype(np.uint8)[..., 3])\n",
    "    display(pil)\n",
    "    pil = Image.fromarray(rgbd_input.numpy().round().astype(np.uint8))\n",
    "    display(pil)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_rgbd(rgbd)\n",
    "rgbd.max(), rgbd.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_depth_limit(rgbd):\n",
    "    depths = rgbd[..., 3]\n",
    "    threshold = depths.max() * 0.05\n",
    "    d_mean = depths[depths > threshold].mean()\n",
    "    return np.ones_like(depths) * d_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_limits = generate_depth_limit(rgbd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def midas_limit_depth(depth_limit):\n",
    "    def callback(i, t, noise_pred_uncond, model_input, target=depth_limit):\n",
    "        # TODO: check this\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            with torch.enable_grad():\n",
    "                ltt: torch.Tensor = model_input.detach()\n",
    "                ltt = ltt.requires_grad_(True)\n",
    "\n",
    "                pred = ltt[:, -1]\n",
    "                mask = (pred > target)\n",
    "                loss_func = torch.nn.MSELoss()  # MSE because we assume it follows a normal distribution\n",
    "                loss = loss_func(pred[mask], target[mask])\n",
    "                loss.backward()\n",
    "\n",
    "            grad = ltt.grad * -0.5 * 3 / (1**2)  # divide sigma^2, if depth distribution follows N(target, sigma)\n",
    "            grad = grad.to(dtype=noise_pred_uncond.dtype)\n",
    "            sqrt_one_minus_alpha_prod = (1 - diffusion_scheduler.alphas_cumprod[t]) ** 0.5\n",
    "            noise_pred_uncond -= sqrt_one_minus_alpha_prod.to(grad) * grad\n",
    "            print(\"grad\", grad.mean(), grad.std(), grad.shape)\n",
    "            print(\"noise_pred_uncond\", noise_pred_uncond.mean(), noise_pred_uncond.std(), noise_pred_uncond.shape)\n",
    "    return callback"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Method 3: extend method 2 to latent diffusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
